<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Lin Chen (ÈôàÊûó)</title>

    <meta name="author" content="Lin Chen">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon"
        href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üí°</text></svg>">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-R22F02T1PZ"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'G-R22F02T1PZ');
    </script>

</head>

<body>
    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Lin Chen (ÈôàÊûó)</name>
                                    </p>
                                    <p>Greetings! I'm currently a PhD student in School of Automation, University of
                                        Science and Technology of China, advised by <a
                                            href="https://scholar.google.co.uk/citations?user=r6CvuOUAAAAJ&hl=en">Prof.
                                            Feng Zhao</a>.
                                        I got a B.E. degree at Anhui University in 2020 and join the high-level vision
                                        group at <a href="https://bivlab123.github.io/">USTC-BIVLab</a>.
                                        And I serve as an research intern in <a
                                            href="https://www.shlab.org.cn">Shanghai AI Laboratory</a> now,
                                        supervised by <a
                                            href="https://scholar.google.com.hk/citations?user=GDvt570AAAAJ&hl=zh-CN">Dr.
                                            Jiaqi Wang</a> and <a
                                            href="https://scholar.google.com/citations?user=moHH480AAAAJ&hl=en">Dr. Pan
                                            Zhang</a>.
                                    </p>
                                    <p>
                                        My research interest includes:
                                    <ul>
                                        <li>image semantic segmentation</li>
                                        <li>domain adaptation/generalization</li>
                                        <li>parameter-efficient fine-tuning</li>
                                        <li><strong>vision-language models</strong></li>
                                    </ul>
                                    <strong>I sincerely welcome discussions and collaborations.
                                    If you're interested, please feel free to reach out to me via email.</strong>
                                    </p>
                                    <p style="text-align:center">
                                        <a href="mailto:chlin@mail.ustc.edu.cn">Email</a> &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?user=-t92FH8AAAAJ&hl=en">Google
                                            Scholar</a> &nbsp/&nbsp
                                        <a href="https://github.com/xiaoachen98/">Github</a> &nbsp/&nbsp
                                        <a href="https://huggingface.co/Lin-Chen">HuggingFace</a> &nbsp/&nbsp
                                        <a href="https://twitter.com/Lin_Chen_98">Twitter</a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:40%;max-width:40%">
                                    <a href="images/photo_circle.jpg"><img style="width:100%;max-width:100%"
                                            alt="profile photo" src="images/photo_circle.jpg" class="hoverZoomLink"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <td style="padding:20px;width:100%;vertical-align:middle">
                                <heading>üìåNews</heading>
                                <p>
                                    [2023-11-22] üî• We release the <a href="https://sharegpt4v.github.io/">ShareGPT4V</a> project, 
                                    comprising <strong>100K</strong> GPT4-Vision-generated captions, <strong>1.2M</strong> high-quality captions, <strong>a general image captioner</strong>, and <strong>a superior large multi-modal model, ShareGPT4V-7B</strong>
                                </p>                                
                                <p>
                                    [2023-07-19] <a href="https://arxiv.org/pdf/2307.09362.pdf">DTP</a> is accepted in
                                    ICCV 2023 and achieves SOTA in night-time and full-time semantic segmentation!
                                </p>
                                <p>
                                    [2023-07-11] We release the <a
                                        href="https://lin-chen.site/projects/freedrag/">FreeDrag</a> framework for more
                                    superior and stable "drag" editing!
                                </p>
                                <p>
                                    [2022-10-20] Our <a
                                        href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/61aa557643ae8709b6a4f41140b2234a-Abstract-Conference.html">DDB</a>
                                    receives the <strong>Spotlight Award</strong> in NeurIPS 2022!
                                </p>
                                <p>
                                    [2022-09-15] <a
                                        href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/61aa557643ae8709b6a4f41140b2234a-Abstract-Conference.html">DDB</a>
                                    is accepted in NeurIPS 2022 and achieves SOTA with ResNet counterparts on the
                                    single-source,
                                    multi-source, and multi-target domain-adaptive semantic segmentation tasks!
                                </p>
                                <p>
                                    [2022-03-02] A discriminator-free adversarial domain adaptation framework <a
                                        href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Reusing_the_Task-Specific_Classifier_as_a_Discriminator_Discriminator-Free_Adversarial_Domain_CVPR_2022_paper.html">DALN</a>
                                    is accepted in CVPR 2022!
                                </p>
                            </td>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>üìñResearch</heading>
                                    <p>
                                        * indicates the equal contribution.
                                        <!--
                                        Representative papers are <span
                                            class="highlight">highlighted</span>.
                                        -->
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/Reins.png" alt="Reins" width="260" height="100">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>Stronger, Fewer, & Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation</strong>
                                    <br>
                                    Zhixiang Wei*, <strong><u>Lin Chen*</u></strong>, Yi Jin*, Xiaoxiao Ma, Tianle Liu, Pengyang Ling, Ben Wang, Huaian Chen, Jinjin Zheng
                                    <br>
                                    <em>Arxiv, 2023</em>
                                    <br>
                                    [<a href="https://arxiv.org/pdf/2312.04265.pdf">paper</a>]
                                    <p>We propose the Reins framework, which efficiently fine-tunes vision foundation models for the 
                                        domain generalized semantic segmentation (DGSS) task with just 1% trainable parameters, 
                                        surprisingly surpassing full parameter fine-tuning. And Reins builds a new SOTA in various DGSS benchmarks.
                                    </p>
                                </td>
                            </tr>    

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/sharegpt4v.png" alt="sharegpt4v" width="260" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>ShareGPT4V: Improving Large Multi-Modal Models with Better Captions</strong>
                                    <br>
                                    <strong><u>Lin Chen*</u></strong>, Jinsong Li*, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, Dahua Lin
                                    <br>
                                    <em>Arxiv, 2023</em>
                                    <br>
                                    [<a href="https://sharegpt4v.github.io/">project page</a>]
                                    [<a href="https://arxiv.org/pdf/2311.12793.pdf">paper</a>]
                                    [<a href="https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V">code</a>]
                                    [<a href="https://huggingface.co/spaces/Lin-Chen/ShareGPT4V-7B">demo</a>]
                                    <p>We propose the ShareGPT4V project, 
                                    comprising <strong>100K</strong> GPT4-Vision-generated captions, 
                                    <strong>1.2M</strong> high-quality captions, 
                                    <strong>a general image captioner</strong>, 
                                    and <strong>a superior large multi-modal model, ShareGPT4V-7B</strong>
                                    </p>
                                </td>
                            </tr>                            

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/dtp.jpg" alt="dtp" width="260" height="140">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>Disentangle then Parse:
                                        Night-time Semantic Segmentation with Illumination Disentanglement</strong>
                                    <br>
                                    Zhixiang Wei*, <strong><u>Lin Chen</u></strong>*, Tao Tu, Huaian Chen, Pengyang
                                    Ling, Yi Jin
                                    <br>
                                    <em>ICCV, 2023</em>
                                    <br>
                                    [<a href="https://arxiv.org/pdf/2307.09362.pdf">paper</a>]
                                    [<a href="https://github.com/w1oves/DTP">code</a>]
                                    <p>We propose a novel nigh-time semantic segmentation paradigm, i.e., disentangle
                                        then parse (<b>DTP</b>),
                                        which explicitly disentangles night-time images into light-invariant reflectance
                                        and light-specific illumination components and then recognizes semantics based
                                        on their adaptive fusion.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="projects/freedrag/resources/overview.png" alt="freedrag" width="260"
                                        height="70">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>FreeDrag: Point Tracking is Not What You Need for Interactive Point-based
                                        Image
                                        Editing</strong>
                                    <br>
                                    Pengyang Ling*, <strong><u>Lin Chen</u></strong>*, Pan Zhang, Huaian Chen, Yi Jin
                                    <br>
                                    <em>Arxiv, 2023</em>
                                    <br>
                                    [<a href="https://arxiv.org/abs/2307.04684">paper</a>]
                                    [<a href="https://github.com/LPengYang/FreeDrag">code</a>]
                                    [<a href="https://lin-chen.site/projects/freedrag/">project page</a>]
                                    [<a href="https://openxlab.org.cn/apps/detail/LPengYang/FreeDrag">demo</a>]
                                    <p>We propose a novel "drag" editing framework called <b>FreeDrag</b>
                                        free of the burden of erroneous point tracking and enables achieving
                                        stable point-based editing in challenging scenarios with similar structures,
                                        fine details, or under multi-point targets.
                                    </p>
                                </td>
                            </tr>

                            <!-- <tr bgcolor="#ffffd0"> -->
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/ddb.png" alt="ddb" width="260" height="90">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>Deliberated Domain Bridging for Domain Adaptive Semantic
                                        Segmentation</strong>
                                    <br>
                                    <strong><u>Lin Chen</u></strong>*, Zhixiang Wei*, Xin Jin*, Huaian Chen, Miao Zheng,
                                    Kai Chen, Yi Jin
                                    <br>
                                    <em>NeurIPS, 2022, <strong><red-text>Spotlight</red-text></strong></em>
                                    <br>
                                    [<a href="https://arxiv.org/pdf/2209.07695.pdf">paper</a>]
                                    [<a href="https://github.com/xiaoachen98/DDB">code</a>]
                                    <p>We leverage the complementary characteristics of the coarse-wise and fine-wise
                                        data mixing techniques
                                        to progressively transfer the knowledge from the source to the target domain.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/daln.png" alt="daln" width="260" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>Reusing the Task-specific Classifier as a Discriminator: Discriminator-free
                                        Adversarial Domain Adaptation</strong>
                                    <br>
                                    <strong><u>Lin Chen</u></strong>*, Huaian Chen*, Zhixiang Wei, Xin Jin, Xiao Tan, Yi
                                    Jin, Enhong Chen
                                    <br>
                                    <em>CVPR, 2022</em>
                                    <br>
                                    [<a
                                        href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Reusing_the_Task-Specific_Classifier_as_a_Discriminator_Discriminator-Free_Adversarial_Domain_CVPR_2022_paper.pdf">paper</a>]
                                    [<a href="https://github.com/xiaoachen98/DALN">code</a>]
                                    <p>We reuse the category classifier as a discriminator to form a discriminator-free
                                        adversarial learning framework.
                                    </p>
                                </td>
                            </tr>

                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <td style="padding:20px;width:100%;vertical-align:middle">
                                <heading>üë®‚ÄçüíªExperience</heading>
                                <p>
                                    [2022-07 ~ Now] Research Intern, Open Algorithm group of <strong>Shanghai AI
                                        Laboratory</strong>.
                                </p>
                                <p>
                                    [2022-03 ~ 2022-06] Computer Vision Intern, <a
                                        href="https://github.com/open-mmlab/mmsegmentation">MMSegmentation</a> team in <a
                                        href="https://github.com/open-mmlab">OpenMMLab</a> group
                                    of <strong>Shanghai AI Laboratory</strong>.
                                </p>
                            </td>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <td style="padding:20px;width:100%;vertical-align:middle">
                                <heading>üèÜAwards</heading>
                                <ul>
                                    <li>National Scholarship Award, PRC, 2022.</li>
                                </ul>
                            </td>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <td style="padding:20px;width:100%;vertical-align:middle">
                                <heading>üìùAcademic Service (Reviewer)</heading>
                                <p>
                                    <li><strong>NeurIPS</strong> 2023</li>
                                    <br>
                                    <li><strong>CVPR</strong> 2023, 2024</li>
                                    <br>
                                    <li><strong>ICCV</strong> 2023</li>
                                    <br>
                                    <li><strong>ICLR</strong> 2024</li>
                                </p>
                            </td>
                    </table>

                    <table table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                        <tbody>
                            <tr>
                                <td width="100%" valign="middle">
                                    <heading>üéìEducation</heading>
                                </td>
                            </tr>
                        </tbody>
        </tbody>
    </table>

    <table table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
            <td width="15%">
                <img src="images/ustc.png" width="80">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>University of Science and Technology of China</strong>, Anhui, China
                <br>
                PhD candidate in Computer Vision (Jan. 2020 to present)
                <br>
            </td>
            <table table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                    <td width="15%">
                        <img src="images/ahu.jpg" width="80">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <strong>Anhui University</strong>, Anhui, China <br>
                        <br>
                        B. Eng in Electronic Information Engineering (2016 to 2020)
                    </td>
                </tbody>
            </table>
        </tbody>
    </table>

    <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
        <tbody>
            <tr>
                <td>
                    <br>
                    <p align="right">
                        <font size="1">
                            Thanks the original template from <a href="https://jonbarron.info/">jonbarron</a> and the
                            modifications made by <a href="https://yujun-shi.github.io/">shi</a>.
                        </font>
                    </p>
                </td>
            </tr>
        </tbody>
    </table>
    </td>
    </tr>
    </table>
</body>

</html>