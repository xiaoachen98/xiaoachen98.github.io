<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Lin Chen (ÈôàÊûó)</title>

    <meta name="author" content="Lin Chen">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon"
        href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üí°</text></svg>">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-R22F02T1PZ"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'G-R22F02T1PZ');
    </script>

</head>

<body>
    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Lin Chen (ÈôàÊûó)</name>
                                    </p>
                                    <p>Greetings! I'm currently a PhD student in School of Automation, University of
                                        Science and Technology of China, advised by <a
                                            href="https://scholar.google.co.uk/citations?user=r6CvuOUAAAAJ&hl=en">Prof.
                                            Feng Zhao</a>.
                                        I got a B.E. degree at Anhui University in 2020 and join the <a href="https://bivlab123.github.io/">USTC-BIVLab</a>.
                                        And I serve as an research intern in <a
                                            href="https://www.shlab.org.cn">Shanghai AI Laboratory</a> now,
                                        supervised by <a
                                            href="https://scholar.google.com/citations?user=FscToE0AAAAJ&hl=en">Dr.
                                            Xiaoyi Dong</a>, <a
                                            href="https://scholar.google.com/citations?user=moHH480AAAAJ&hl=en">Dr. Pan
                                            Zhang</a> and  <a
                                            href="https://scholar.google.com.hk/citations?user=GDvt570AAAAJ&hl=en">Dr.
                                            Jiaqi Wang</a>.
                                    </p>
                                    <p>
                                        My research interest includes:
                                    <ul>
                                        <li>image semantic segmentation</li>
                                        <li>transfer learning and parameter-efficient fine-tuning</li>
                                        <li><strong>vision-language models</strong></li>
                                    </ul>
                                    <strong>I sincerely welcome discussions and collaborations.
                                    If you're interested, please feel free to reach out to me via email or WeChat (xiaoachen98).</strong>
                                    </p>
                                    <p style="text-align:center">
                                        <a href="mailto:chlin@mail.ustc.edu.cn">Email</a> &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?user=-t92FH8AAAAJ&hl=en">Google
                                            Scholar</a> &nbsp/&nbsp
                                        <a href="https://github.com/xiaoachen98/">Github</a> &nbsp/&nbsp
                                        <a href="https://huggingface.co/Lin-Chen">HuggingFace</a> &nbsp/&nbsp
                                        <a href="https://twitter.com/Lin_Chen_98">Twitter</a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:40%;max-width:40%">
                                    <a href="images/photo_circle.jpg"><img style="width:100%;max-width:100%"
                                            alt="profile photo" src="images/photo_circle.jpg" class="hoverZoomLink"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <td style="padding:20px;width:100%;vertical-align:middle">
                                <heading>üìåNews</heading>
                                <p>
                                    [2024.8] Thrilled that I've won third place on the July 2024 <a href="https://huggingface.co/spaces/mvaloatto/TCTF"> Most Impactful Users </a>leaderboard on HuggingFace.
                                </p>                                 
                                <p>
                                    [2024.7] Happy to announce that <a href="https://sharegpt4v.github.io/">ShareGPT4V</a> was accepted by ECCV 2024!
                                </p>  
                                <p>
                                    [2024.6] üî• We release <a href="https://sharegpt4video.github.io/">ShareGPT4Video</a>, 
                                    comprising <strong>40K</strong> GPT4V-generated captions, <strong>4.8M</strong> high-quality captions, <strong>a general video captioner</strong>, and <strong>a superior large multi-modal model, ShareGPT4Video-8B</strong>
                                </p>  
                                <p>
                                    [2024.5] We release <a href="https://github.com/xiaoachen98/Open-LLaVA-NeXT">Open-LLaVA-Next</a>, 
                                    an open-source implementation of LLaVA-NeXT series for facilitating the large multi-modal model community. 
                                    All training data and checkpoints at each stage are open-sourced, and friendly for research usage.
                                </p>   
                                <p>
                                    [2024.4] We release <a href="https://arxiv.org/pdf/2403.20330.pdf">MMStar</a>, 
                                    an elite vision-indispensable multi-modal benchmark.
                                </p>                                 
                                <p>
                                    [2024.3] Two papers <a href="https://arxiv.org/pdf/2312.04265.pdf">Rein</a> and
                                     <a href="https://lin-chen.site/projects/freedrag/">FreeDrag</a> were accepted in CVPR 2024!
                                </p>   
                                <p>
                                    [2023.11] üî• We release the <a href="https://sharegpt4v.github.io/">ShareGPT4V</a> project, 
                                    comprising <strong>100K</strong> GPT4-Vision-generated captions, <strong>1.2M</strong> high-quality captions, <strong>a general image captioner</strong>, and <strong>a superior large multi-modal model, ShareGPT4V-7B</strong>
                                </p>                                
                                <p>
                                    [2023.7] <a href="https://arxiv.org/pdf/2307.09362.pdf">DTP</a> is accepted in
                                    ICCV 2023 and achieves SOTA in night-time and full-time semantic segmentation!
                                </p>
                                <p>
                                    [2023.7] We release the <a
                                        href="https://lin-chen.site/projects/freedrag/">FreeDrag</a> framework for more
                                    superior and stable "drag" editing!
                                </p>
                                <p>
                                    [2022.10] Our <a
                                        href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/61aa557643ae8709b6a4f41140b2234a-Abstract-Conference.html">DDB</a>
                                    receives the <strong>Spotlight Award</strong> in NeurIPS 2022!
                                </p>
                                <p>
                                    [2022.9] <a
                                        href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/61aa557643ae8709b6a4f41140b2234a-Abstract-Conference.html">DDB</a>
                                    is accepted in NeurIPS 2022 and achieves SOTA with ResNet counterparts on the
                                    single-source,
                                    multi-source, and multi-target domain-adaptive semantic segmentation tasks!
                                </p>
                                <p>
                                    [2022.3] A discriminator-free adversarial domain adaptation framework <a
                                        href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Reusing_the_Task-Specific_Classifier_as_a_Discriminator_Discriminator-Free_Adversarial_Domain_CVPR_2022_paper.html">DALN</a>
                                    is accepted in CVPR 2022!
                                </p>
                            </td>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <td style="padding:20px;width:100%;vertical-align:middle">
                                <heading>üë®‚ÄçüíªExperience</heading>
                                <p>
                                    [2022-07 ~ Now] Research Intern, Open Algorithm group of <strong>Shanghai AI
                                        Laboratory</strong>.
                                </p>
                                <p>
                                    [2022-03 ~ 2022-06] Computer Vision Intern, <a
                                        href="https://github.com/open-mmlab/mmsegmentation">MMSegmentation</a> team in <a
                                        href="https://github.com/open-mmlab">OpenMMLab</a> group
                                    of <strong>Shanghai AI Laboratory</strong>.
                                </p>
                            </td>
                    </table>                    

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>üìñResearch</heading>
                                    <p>
                                        <strong>* indicates the equal contribution.</strong>
                                        <!--
                                        Representative papers are <span
                                            class="highlight">highlighted</span>.
                                        -->
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/ixc-2-5.png" alt="ixc-2-5" width="260" height="260">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output</strong>
                                    <br>
                                    Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, <strong><u>Lin Chen</u></strong>, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, Jiaqi Wang
                                    <br>
                                    <em>Arxiv, 2024</em>
                                    <br>
                                    [<a href="https://arxiv.org/pdf/2407.03320">paper</a>]
                                    [<a href="https://huggingface.co/spaces/Willow123/InternLM-XComposer">demo</a>]
                                    [<a href="https://github.com/InternLM/InternLM-XComposer">code</a>]
                                    <iframe src="https://ghbtns.com/github-btn.html?user=InternLM&repo=InternLM-XComposer&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
                                    <p>A versatile large-vision language model that supports <strong>long-contextual input and output</strong>. IXC-2.5 excels in various text-image comprehension and composition applications, <strong>achieving GPT-4V level capabilities with merely 7B LLM backend</strong>.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/sharegpt4v.png" alt="sharegpt4v" width="260" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>ShareGPT4V: Improving Large Multi-Modal Models with Better Captions</strong>
                                    <br>
                                    <strong><u>Lin Chen*</u></strong>, Jinsong Li*, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, Dahua Lin
                                    <br>
                                    <em>ECCV, 2024</em>
                                    <br>
                                    [<a href="https://sharegpt4v.github.io/">project page</a>]
                                    [<a href="https://arxiv.org/pdf/2311.12793.pdf">paper</a>]
                                    [<a href="https://huggingface.co/spaces/Lin-Chen/ShareGPT4V-7B">demo</a>]
                                    [<a href="https://github.com/ShareGPT4Omni/ShareGPT4V">code</a>]
                                    <iframe src="https://ghbtns.com/github-btn.html?user=InternLM&repo=InternLM-XComposer&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
                                    <p>We propose the ShareGPT4V project, 
                                    comprising <strong>100K</strong> GPT4-Vision-generated captions, 
                                    <strong>1.2M</strong> high-quality captions, 
                                    <strong>a general image captioner</strong>, 
                                    and <strong>a superior large multi-modal model, ShareGPT4V-7B</strong>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/prism.jpg" alt="prism" width="260" height="140">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs</strong>
                                    <br>
                                    Yuxuan Qiao, Haodong Duan, Xinyu Fang, Junming Yang, <strong><u>Lin Chen</u></strong>, Songyang Zhang, Jiaqi Wang, Dahua Lin, Kai Chen
                                    <br>
                                    <em>Arxiv, 2024</em>
                                    <br>
                                    [<a href="https://arxiv.org/pdf/2406.14544">paper</a>]
                                    [<a href="https://github.com/SparksJoe/Prism">code</a>]
                                    <iframe src="https://ghbtns.com/github-btn.html?user=SparksJoe&repo=Prism&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
                                    <p>
                                        We present Prism, an innovative framework designed to <strong>disentangle the perception and reasoning</strong> processes involved in visual question solving. 
                                    </p>
                                </td>
                            </tr>
                            
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/sharegpt4video.png" alt="ShareGPT4Video" width="260" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>ShareGPT4Video: Improving Video Understanding and Generation with Better Captions</strong>
                                    <br>
                                    <strong><u>Lin Chen*</u></strong>, Xilin Wei*, Jinsong Li*, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, Jiaqi Wang
                                    <br>
                                    <em>Arxiv, 2024</em>
                                    <br>
                                    [<a href="https://sharegpt4video.github.io/">project page</a>]
                                    [<a href="https://arxiv.org/pdf/2406.04325v1">paper</a>]
                                    [<a href="https://github.com/ShareGPT4Omni/ShareGPT4Video">code</a>]
                                    <iframe src="https://ghbtns.com/github-btn.html?user=ShareGPT4Omni&repo=ShareGPT4Video&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
                                    <p>
                                        A large-scale highly descriptive video-text dataset, with 40K captions annotated by GPT4V and 4.8M captions annotated by our ShareCaptioner-Video. The total videos last with 300 hours and 3000 hours separately!
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/mmstar.png" alt="Reins" width="260" height="150">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>Are We on the Right Way for Evaluating Large Vision-Language Models?</strong>
                                    <br>
                                    <strong><u>Lin Chen*</u></strong>, Jinsong Li*, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, Feng Zhao
                                    <br>
                                    <em>Arxiv, 2024</em>
                                    <br>
                                    [<a href="https://mmstar-benchmark.github.io/">project page</a>]
                                    [<a href="https://arxiv.org/abs/2403.20330">paper</a>]
                                    [<a href="https://github.com/MMStar-Benchmark/MMStar">code</a>]
                                    <iframe src="https://ghbtns.com/github-btn.html?user=MMStar-Benchmark&repo=MMStar&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
                                    <p>We identify two primary issues in existing evaluation studies for large vision-language models. We further
                                        develop an elite vision-indispensable multi-modal benchmark and two novel metrics to measure data leakage and actual performance gain in multi-modal training.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/Reins.png" alt="Reins" width="260" height="110">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>Stronger, Fewer, & Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation</strong>
                                    <br>
                                    Zhixiang Wei*, <strong><u>Lin Chen*</u></strong>, Yi Jin*, Xiaoxiao Ma, Tianle Liu, Pengyang Ling, Ben Wang, Huaian Chen, Jinjin Zheng
                                    <br>
                                    <em>CVPR, 2024</em>
                                    <br>
                                    [<a href="https://arxiv.org/pdf/2312.04265.pdf">paper</a>]
                                    [<a href="https://github.com/w1oves/Rein">code</a>]
                                    <iframe src="https://ghbtns.com/github-btn.html?user=w1oves&repo=Rein&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
                                    <p>We propose the Rein framework, which efficiently fine-tunes vision foundation models for the 
                                        domain generalized semantic segmentation (DGSS) task with just 1% trainable parameters, 
                                        surprisingly surpassing full parameter fine-tuning. And Reins builds a new SOTA in various DGSS benchmarks.
                                    </p>
                                </td>
                            </tr>    

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="projects/freedrag/resources/overview.png" alt="freedrag" width="260"
                                        height="80">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>FreeDrag: Point Tracking is Not What You Need for Interactive Point-based
                                        Image
                                        Editing</strong>
                                    <br>
                                    Pengyang Ling*, <strong><u>Lin Chen</u></strong>*, Pan Zhang, Huaian Chen, Yi Jin
                                    <br>
                                    <em>CVPR, 2024</em>
                                    <br>
                                    [<a href="https://lin-chen.site/projects/freedrag/">project page</a>]
                                    [<a href="https://arxiv.org/abs/2307.04684">paper</a>]
                                    [<a href="https://openxlab.org.cn/apps/detail/LPengYang/FreeDrag">demo</a>]
                                    [<a href="https://github.com/LPengYang/FreeDrag">code</a>]
                                    <iframe src="https://ghbtns.com/github-btn.html?user=LPengYang&repo=FreeDrag&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
                                    <p>We propose a novel "drag" editing framework called <b>FreeDrag</b>
                                        free of the burden of erroneous point tracking and enables achieving
                                        stable point-based editing in challenging scenarios with similar structures,
                                        fine details, or under multi-point targets.
                                    </p>
                                </td>
                            </tr>              

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/dtp.jpg" alt="dtp" width="260" height="140">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>Disentangle then Parse:
                                        Night-time Semantic Segmentation with Illumination Disentanglement</strong>
                                    <br>
                                    Zhixiang Wei*, <strong><u>Lin Chen</u></strong>*, Tao Tu, Huaian Chen, Pengyang
                                    Ling, Yi Jin
                                    <br>
                                    <em>ICCV, 2023</em>
                                    <br>
                                    [<a href="https://arxiv.org/pdf/2307.09362.pdf">paper</a>]
                                    [<a href="https://github.com/w1oves/DTP">code</a>]
                                    <iframe src="https://ghbtns.com/github-btn.html?user=w1oves&repo=DTP&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
                                    <p>We propose a novel nigh-time semantic segmentation paradigm, i.e., disentangle
                                        then parse (<b>DTP</b>),
                                        which explicitly disentangles night-time images into light-invariant reflectance
                                        and light-specific illumination components and then recognizes semantics based
                                        on their adaptive fusion.
                                    </p>
                                </td>
                            </tr>

                            <!-- <tr bgcolor="#ffffd0"> -->
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/ddb.png" alt="ddb" width="260" height="90">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>Deliberated Domain Bridging for Domain Adaptive Semantic
                                        Segmentation</strong>
                                    <br>
                                    <strong><u>Lin Chen</u></strong>*, Zhixiang Wei*, Xin Jin*, Huaian Chen, Miao Zheng,
                                    Kai Chen, Yi Jin
                                    <br>
                                    <em>NeurIPS, 2022, <strong><red-text>Spotlight</red-text></strong></em>
                                    <br>
                                    [<a href="https://arxiv.org/pdf/2209.07695.pdf">paper</a>]
                                    [<a href="https://github.com/xiaoachen98/DDB">code</a>]
                                    <iframe src="https://ghbtns.com/github-btn.html?user=xiaoachen98&repo=DDB&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
                                    <p>We leverage the complementary characteristics of the coarse-wise and fine-wise
                                        data mixing techniques
                                        to progressively transfer the knowledge from the source to the target domain.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/daln.png" alt="daln" width="260" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <strong>Reusing the Task-specific Classifier as a Discriminator: Discriminator-free
                                        Adversarial Domain Adaptation</strong>
                                    <br>
                                    <strong><u>Lin Chen</u></strong>*, Huaian Chen*, Zhixiang Wei, Xin Jin, Xiao Tan, Yi
                                    Jin, Enhong Chen
                                    <br>
                                    <em>CVPR, 2022</em>
                                    <br>
                                    [<a
                                        href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Reusing_the_Task-Specific_Classifier_as_a_Discriminator_Discriminator-Free_Adversarial_Domain_CVPR_2022_paper.pdf">paper</a>]
                                    [<a href="https://github.com/xiaoachen98/DALN">code</a>]
                                    <iframe src="https://ghbtns.com/github-btn.html?user=xiaoachen98&repo=DALN&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
                                    <p>We reuse the category classifier as a discriminator to form a discriminator-free
                                        adversarial learning framework.
                                    </p>
                                </td>
                            </tr>

                        </tbody>
                    </table>
        
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <td style="padding:20px;width:100%;vertical-align:middle">
                                <heading>üîßProjects</heading>
                                <ul>
                                    <li><strong>Open-LLaVA-NeXT:</strong> an open-source implementation of LLaVA-NeXT <iframe src="https://ghbtns.com/github-btn.html?user=xiaoachen98&repo=Open-LLaVA-NeXT&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe></li>
                                    <li><strong>VLMEvalKit:</strong> open-source evaluation toolkit of large vision-language models <iframe src="https://ghbtns.com/github-btn.html?user=open-compass&repo=VLMEvalKit&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe></li>
                                    <li><strong>InternLM-XComposer:</strong>  a series of large multi-modal models in the InternLM group <iframe src="https://ghbtns.com/github-btn.html?user=InternLM&repo=InternLM-XComposer&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe></li>
                                    <li><strong>MMSegmentation:</strong> an open source semantic segmentation toolbox based on PyTorch <iframe src="https://ghbtns.com/github-btn.html?user=open-mmlab&repo=mmsegmentation&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe></li>
                                </ul>
                            </td>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <td style="padding:20px;width:100%;vertical-align:middle">
                                <heading>üèÜAchievements</heading>
                                <ul>
                                    <li>500+ Citations on Google Scholar</li>
                                </ul>
                                <ul>
                                    <li>The Third Place on the July 2024 Most Impactful Users of HuggingFace</li>
                                </ul>
                                <ul>
                                    <li>14M+ Model Downloads on HuggingFace</li>
                                </ul>
                                <ul>
                                    <li>National Scholarship Award, PRC, 2022.</li>
                                </ul>
                            </td>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <td style="padding:20px;width:100%;vertical-align:middle">
                                <heading>üìùAcademic Service (Reviewer)</heading>
                                <p>
                                    <li><strong>NeurIPS</strong> 2023, 2024</li>
                                    <br>
                                    <li><strong>CVPR</strong> 2023, 2024</li>
                                    <br>
                                    <li><strong>ICCV</strong> 2023</li>
                                    <br>
                                    <li><strong>ECCV</strong> 2024</li>
                                    <br>
                                    <li><strong>ICLR</strong> 2024</li>
                                </p>
                            </td>
                    </table>

                    <table table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                        <tbody>
                            <tr>
                                <td width="100%" valign="middle">
                                    <heading>üéìEducation</heading>
                                </td>
                            </tr>
                        </tbody>
        </tbody>
    </table>

    <table table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
            <td width="15%">
                <img src="images/ustc.png" width="80">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>University of Science and Technology of China</strong>, Anhui, China
                <br>
                PhD candidate in Computer Vision (Jan. 2020 to present)
                <br>
            </td>
            <table table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                    <td width="15%">
                        <img src="images/ahu.jpg" width="80">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <strong>Anhui University</strong>, Anhui, China <br>
                        <br>
                        B. Eng in Electronic Information Engineering (2016 to 2020)
                    </td>
                </tbody>
            </table>
        </tbody>
    </table>

    <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
        <tbody>
            <tr>
                <td>
                    <br>
                    <p align="right">
                        <font size="1">
                            Thanks the original template from <a href="https://jonbarron.info/">jonbarron</a> and the
                            modifications made by <a href="https://yujun-shi.github.io/">shi</a>.
                        </font>
                    </p>
                </td>
            </tr>
        </tbody>
    </table>
    </td>
    </tr>
    </table>
</body>

</html>
